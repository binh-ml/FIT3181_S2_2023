{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">  FIT3181: Deep Learning (2023)</span>\n",
    "\n",
    "***\n",
    "*CE/Lecturers:*  **Prof Dinh Phung** \\[dinh.phung@monash.edu\\] | **Prof Jianfei Cai** \\[jainfei.cai@monash.edu\\] | \n",
    "**Dr Toan Do** \\[toan.do@monash.edu\\] | **Dr Lim-Chern Hong** \\[lim.chernhong@monash.edu\\]\n",
    "<br/>\n",
    "\n",
    "*Tutors:*  **Dr Binh Nguyen** \\[binh.nguyen1@monash.edu \\] | **Mr Tony Bui** \\[tuan.bui@monash.edu\\] | **Ms Vy Vo** \\[v.vo@monash.edu \\]\n",
    "<br/>\n",
    "\n",
    "Faculty of Information Technology, Monash University, Australia\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">Lab 02c: Feedforward Neural Nets with TensorFlow 1.x</span> \n",
    "**The purpose of this tutorial is to demonstrate an end-to-end process of building and training a Feedforward Neural Network with TensorFlow. In this tutorial, you will learn**:  \n",
    "- What constitutes a deep learning pipeline.\n",
    "- How to implement a feedforward neural net for a multi-class classfication problem using TF 1.x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\"> Feedforward Neural Network </span> <span style=\"color:red\"></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will consider a fairly realistic deep NNs with *three* layers plus the *output* layer. Its architecture will be specified as: $16 \\rightarrow 10 (ReLU) \\rightarrow 20 (ReLU) \\rightarrow 15 (ReLu) \\rightarrow 26$. This means:\n",
    "- Input size is 16\n",
    "- First layer has 10 hidden units with ReLU activation functions\n",
    "- Second layer has 20 hidden units with 20 ReLU activiation functions\n",
    "- Third layer has 15 hidden units with 15 ReLU activiation functions\n",
    "- And output layer is logit layer with 26 hidden units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network, for example, can take the `letter` dataset with an input of $16$ features and target label of $26$ classes (A-Z). **Our objective in this tutorial is to implement this specific network in `TensorFlow 1.x`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">I. Specifying the Neural Network Architecture </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize this network as in the figure below. Please note that for readability, the number of hidden units in the figure might not correspond exactly to the actual size of the hidden units being used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/DNN_Pipeline.PNG\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure shows the pipeline of the entire process of feeding a mini-batch of size $32$ into the network. It is Note that using***mini-batch*** is a common way to train deep NNs in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us denote the mini-batch by $X_b= \\{(x_1, y_1),\\dots, (x_{32}, y_{32})\\}$. The mini-batch can be stored using a $2D$ tensor with the shape $(32, 16)$. Assume that in this network, we use the activation function $ReLu$ where $ReLu(t)= \\max\\{0, t\\}$. The computation in the forward propagation step is as follows:\n",
    "- Input $X_b$ with mini-batch size of 32\n",
    "- $h_1= ReLu(X_b \\times W^1 + b^1)\\in \\mathbb{R}^{32 \\times 10}$. \n",
    "- $h_2= ReLu(h_1 \\times W^2 + b^2\\in \\mathbb{R}^{32 \\times 20}$. \n",
    "- $h_3= ReLu(h_2 \\times W^3 + b^3\\in \\mathbb{R}^{32 \\times 15}$. \n",
    "- $logits= h_3 \\times W^4 + b^4 \\in \\mathbb{R}^{32 \\times 26}$\n",
    "- $p = softmax(logits) \\in \\mathbb{R}^{32 \\times 26}$ <br>\n",
    "\n",
    "where we note that the activation function is perfomed element-wise and the softmax function is used to transform a vector of scalars to a discrete distribution as: \n",
    "\n",
    "$$softmax(z)=\\big[\\frac{\\exp(z_i)}{\\sum_{j=1}^{26}{\\exp(z_j)}}\\big]_{i=1}^{26}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "The $k$-th row $p_k$ of the matrix $p$ can represent the probability distribution to classify the data point $x_k$ to the classes $1,2,\\dots,26$. In particular, we have:\n",
    "\n",
    "$$p_{km}= p(y_k=m \\mid x_k)  \\text{ for }  m=1,2,\\dots,26$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\"> Exercise 1</span>** : Explain why the dimension for $h_1$ is $32\\times 10$? Similarly, please work out the dimension for $h2, h3, logits$ and $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">Specifying the Loss Function </span>\n",
    "Essiential to training a deep NN is the concept of the **loss function**. This function will tell us how good the network is predicting, hence we can use this loss to find the network weights in such a way that the loss can be minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification task, a common approach is to use the **cross-entropy** loss function. Given a data-label instance $(x_k,y_k)$ where feature $x_k\\in \\mathbb{R}^{16}$ and the label $y_k \\in \\{1,2,...,26\\}$ is a numeric label (for example if $x_k$ is in the class 2, then $y_k =2 $ and its one-hot vector $1_{y_k}=[0,1,0,...,0]$). The cross-entropy between the classification distribution $p_k$ returned from the NN and true label distribution $y_k$ is defined as:\n",
    "\n",
    "$$cross\\_entropy(1_{y_k}, p_k)=-\\sum_{j=1}^{26}y_{kj}\\log{p_{kj}}=-\\log p_{k,y_k}.$$\n",
    "\n",
    "This loss basically enforces the model to predict the label as close as the true label by minimizing $cross\\_entropy(1_{y_k}, p_k)$.\n",
    "\n",
    "The above loss function was applied for each instance. For the entire current mini-batch, our loss function becomes: \n",
    "$$\\min \\sum_{k=1}^{32}cross\\_entropy(1_{y_k}, p_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\"> Exercise 2: </span>** : **<span style=\"color:#0b486b\">In the corss-entropy equation above, $y_k$ is the class for $x_k$, explain why the end result is $-\\log p_{k,y_k}$.</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\"> Exercise 3: </span>** : **<span style=\"color:#0b486b\">Let $p=[0.1, 0.3, 0.6]$ and $q=[0.0, 0.5, 0.5]$ be two discrete distributions, what is the $cross\\_entropy(q,p)$ ?</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\"> II. Implementation with TensorFlow 1.x</span> <span style=\"color:red\">**** (important)</span>\n",
    "We now implement the aforementioned network with the architecture of $16 \\rightarrow 10 (ReLU) \\rightarrow 20 (ReLU) \\rightarrow 15 (ReLu) \\rightarrow 26$ in Tensorflow. We will experiment with using the `letter` dataset, which can be found at [the LIBSVM website](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html#letter). \n",
    "\n",
    "*The objective is to identify each of a large number of black-and-white rectangular pixel displays as one of the 26 capital letters in the English alphabet. The character images were based on 20 different fonts and each letter within these 20 fonts was randomly distorted to produce a file of 20,000 unique stimuli. Each stimulus was converted into 16 primitive numerical attributes (statistical moments and edge counts) which were then scaled to fit into a range of integer values from 0 through 15*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard process of implementing a deep learning model is described as follows:\n",
    "\n",
    "**Step 1: Data processing** \n",
    "   - Load the dataset and split into train, valid, and test sets.  \n",
    "\n",
    "\n",
    "**Step 2: Construction phase** \n",
    "   - Define the NN model and construct the corresponding computational graph.\n",
    "   - Define the loss function and the relevant measures of performance of interest (accuracy, F1, and AUC).\n",
    "\n",
    "\n",
    "**Step 3: Execution and evaluation phase**\n",
    "   - Train the model using mini-batches from the train set by minimizing the loss function with an optimizer.\n",
    "   - Predict on the test set and access its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">Step 1: Data Processing </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first use `sklearn` to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_svmlight_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_name= \"letter_scale.libsvm\"\n",
    "data_file = os.path.abspath(\"./Data/\" + data_file_name)\n",
    "X_data, y_data = load_svmlight_file(data_file)\n",
    "X_data = X_data.toarray()\n",
    "y_data = y_data.reshape(y_data.shape[0],-1)\n",
    "print(\"X data shape: {}\".format(X_data.shape))\n",
    "print(\"y data shape: {}\".format(y_data.shape))\n",
    "print(\"# classes: {}\".format(len(np.unique(y_data))))\n",
    "print(np.unique(y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now split the dataset into the train, validation, and test sets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "def train_valid_test_split(data, target, train_size, test_size):\n",
    "    valid_size = 1 - (train_size + test_size)\n",
    "    X1, X_test, y1, y_test = train_test_split(data, target, test_size = test_size, random_state= 33)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X1, y1, test_size = float(valid_size)/(valid_size+ train_size))\n",
    "    return X_train, X_valid, X_test, y_train, y_valid, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we would like to encode the label in the form of numeric vector. For example, we want to turn $y\\_data=[\"cat\", \"dog\", \"cat\", \"lion\", \"dog\"]$ to $y\\_data=[0,1,0,2,1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, in the following segment of code, we use the object `le` as an instance of the class `preprocessing.LabelEncoder()` which transforms the catefgorial labels in `y_data` into a numerical vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_data)\n",
    "y_data = le.transform(y_data)\n",
    "print(y_data[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the function defined above to prepare our data for training, validating and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, X_test, y_train, y_valid, y_test = train_valid_test_split(X_data, y_data, train_size=0.8, test_size=0.1)\n",
    "y_train = y_train.reshape(-1)\n",
    "y_test = y_test.reshape(-1)\n",
    "y_valid = y_valid.reshape(-1)\n",
    "print(X_train.shape, X_valid.shape, X_test.shape)\n",
    "print(y_train.shape, y_valid.shape, y_test.shape)\n",
    "print(\"lables: {}\".format(np.unique(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store some information about the training set for later uses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(X_train.shape[0])\n",
    "n_features = int(X_train.shape[1])\n",
    "n_classes = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent (SGD) is a commonly used optimizer in real-world implementation of deep learning models. Input to this algorithm is a sequence of **mini-batch** of data drawn from the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">Step 2: Construction Phase </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build up a feedforward neural network with the architecture: $16 \\rightarrow 10 (ReLU) \\rightarrow 20 (ReLU) \\rightarrow 15 (ReLu) \\rightarrow 26$ in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in = n_features    # dimension of input\n",
    "n1 = 10              # number of hidden units at the first layer\n",
    "n2 = 20              # number of hidden units at the second layer\n",
    "n3 = 15              # number of hidden units at the third layer\n",
    "n_out = n_classes    # number of classification classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `dense_layer` represents a fully connected layer in a deep learning network. This takes $W,b$ and input as inputs and returns $\\sigma(W \\times input + b)$ where the activation function $\\sigma$ is specified by the parameter `act`.\n",
    "\n",
    "`TensorFlow` provides many options for `activation functions` such as `tf.nn.relu`, `tf.nn.sigmoid`, `tf.nn.tanh`. You can also define your own activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_layer(inputs, output_size, act=None, name=\"hidden-layer\"):\n",
    "    with tf.name_scope(name):\n",
    "        input_size = int(inputs.get_shape()[1])\n",
    "        W_init = tf.random.normal([input_size, output_size], mean=0, stddev= 0.1, dtype= tf.float32)\n",
    "        b_init = tf.random.normal([output_size], mean=0, stddev= 0.1, dtype= tf.float32)\n",
    "        W = tf.Variable(W_init, name= \"W\")\n",
    "        b = tf.Variable(b_init, name=\"b\")\n",
    "        Wxb = tf.matmul(inputs, W) + b\n",
    "        if act is None:\n",
    "            return Wxb\n",
    "        else:\n",
    "            return act(Wxb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now construct the computational graph. But before that, remember to reset the default graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.name_scope(\"network\"):\n",
    "    X = tf.placeholder(shape=[None, n_in], dtype= tf.float32)\n",
    "    y = tf.placeholder(shape=[None], dtype= tf.int32)\n",
    "    h1 = dense_layer(X, n1, act= tf.nn.relu, name= \"layer1\")\n",
    "    h2 = dense_layer(h1, n2, act= tf.nn.relu, name= \"layer2\")\n",
    "    h3 = dense_layer(h2, n3, act= tf.nn.relu, name= \"layer3\")\n",
    "    logits = dense_layer(h3, n_out, name=\"logits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the cross-entropy loss. In TensorFlow, you can use two of the following functions for evaluating the cross-entropy loss:\n",
    "- `tf.nn.sparse_softmax_cross_entropy_with_logits`: if the labels `y_train` is in the categorial format (e.g., `y_train=[0,1,0,1,1,2]`).\n",
    "- `tf.nn.softmax_cross_entropy_with_logits`: if the labels `y_train` is in the one-hot format (e.g., `y_train=[[1,0,0], [0,1,0], [1,0,0], [0,0,1]]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to specify an optimizer to minimize the loss. Here, we are using the Adam optimizer for this optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('train'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, \n",
    "                                                              logits=logits, \n",
    "                                                              name='xentropy')\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    tf.summary.scalar(\"loss\", loss)    # summarize the loss\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, we run  to add the loss value the summary.\n",
    "\n",
    "To compute the prediction accuracy of our model, you can use the function [`in_top_k()`](https://www.tensorflow.org/api_docs/python/tf/nn/in_top_k) with `k = 1`. This returns a 1D tensor full of boolean values, so we need to cast these booleans to floats and then compute the average. This will give us the networkâ€™s overall accuracy. \n",
    "\n",
    "The lines `tf.summary.scalar(\"loss\", loss)` and `tf.summary.scalar(\"accuracy\", accuracy)` are used to add the loss and accuracy values respectively to the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('evaluation'):\n",
    "    correct = tf.nn.in_top_k(logits, y, k = 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)  #summarize the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define two FileWriters to write the summary to two log folders. In this way, we can plot the train, valid losses (or accuracies) on the same graph. Note that you can use this trick whenever you wish to display multiple plots on the same graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not os.path.exists(\"./logs/train\")):\n",
    "    os.makedirs(\"./logs/train\")\n",
    "\n",
    "if(not os.path.exists(\"./logs/val\")):\n",
    "    os.makedirs(\"./logs/val\")\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(\"./logs/train\")\n",
    "valid_writer = tf.summary.FileWriter(\"./logs/val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">Step 3: Execution and Evaluation Phase </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `execution phase`, we need to create a `TensorFlow session`, then initialize `all variables` in the graph, execute `train_op`, and query the values of necessary nodes (e.g., `loss` and `accuracy`). The corresponding methods for each step are given as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialize all variables\n",
    "  - `init = tf.global_variables_initializer()` and `sess.run(init)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Execute `train_op` when feeding mini-batches to the network\n",
    "  - `sess.run([train_op], feed_dict={X:x_batch, y:y_batch})`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Query the values of necessary nodes\n",
    "  - `val_loss, val_accuracy = sess.run([loss, accuracy], feed_dict={X:X_valid, y:y_valid})`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that as a rule of machine learning, we **must not** use the `test set` during the training phase and **only use** this set for evaluating the predictive performance of a trained model. \n",
    "\n",
    "Putting everything altogether, the following snippet demonstrates the training process of our feedforward neural network. \n",
    "\n",
    "The second last line `test_accuracy = sess.run(accuracy, feed_dict={X:X_test, y:y_test})` is to output the predictive performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "batch_size = 32\n",
    "history = []  #used to store train, valid accuracies and losses for showing later\n",
    "num_epoch = 100\n",
    "iter_per_epoch= math.ceil(float(train_size)/batch_size)  #number of iterations per epoch\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epoch):\n",
    "        for idx_start in range(0, X_train.shape[0], batch_size):\n",
    "            idx_end = min(X_train.shape[0], idx_start + batch_size)\n",
    "            X_batch, y_batch = X_train[idx_start:idx_end], y_train[idx_start:idx_end]\n",
    "            sess.run([train_op], feed_dict={X:X_batch, y:y_batch})\n",
    "        #compute accuracies and losses at the end of each epoch\n",
    "        train_summary, train_loss, train_accuracy= sess.run([merged,loss, accuracy], feed_dict={X:X_train, y:y_train})\n",
    "        train_writer.add_summary(train_summary, epoch +1)\n",
    "        train_writer.flush()\n",
    "        \n",
    "        valid_summary,val_loss, val_accuracy= sess.run([merged,loss, accuracy], feed_dict={X:X_valid, y:y_valid})\n",
    "        valid_writer.add_summary(valid_summary, epoch +1)\n",
    "        valid_writer.flush()\n",
    "        print(\"Epoch {}: valid loss={:.4f}, valid acc={:.4f}\".format(epoch+1, val_loss, val_accuracy))\n",
    "        print(\"########: train loss={:.4f}, train acc={:.4f}\".format(train_loss, train_accuracy))\n",
    "        hist_item={\"train_loss\": train_loss, \"train_acc\": train_accuracy, \n",
    "                   \"val_loss\":val_loss, \"val_acc\": val_accuracy}\n",
    "        history.append(hist_item)\n",
    "    print(\"---------------------------------------------\\n\")\n",
    "    test_accuracy = sess.run(accuracy, feed_dict={X:X_test, y:y_test})\n",
    "    print(\"Test accuracy: {:.4f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\"> Additional Exercises </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">Exercise 1:</span>** Write your own code to save a trained model to the hard disk and restore this model, then use the restored model to output the prediction result on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">Exercise 2:</span>** Write code to add the plots of `test accuracy` and `loss` to the above line charts with your color of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">Exercise 3:</span>** Insert new code to the above code to enable outputting to TensorBoard the values of `training loss`, `training accuracy`, `valid loss`, and `valid accuracy` at the end of epochs. You can refer to the code [here](https://www.tensorflow.org/guide/summaries_and_tensorboard)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">Exercise 4:</span>** Write code to do regression on the dataset `cadata` which can be downloaded [here](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression.html). Note that for a regression problem, you need to use the `L2` loss instead of the `cross-entropy` loss as in a classification problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <span style=\"color:#0b486b\"> <div  style=\"text-align:center\">**THE END**</div> </span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fit3181",
   "language": "python",
   "name": "fit3181"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
